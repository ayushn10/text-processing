{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushn10/text-processing/blob/main/text_proc_proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QxhoqlihTgW"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(\"text proc.txt\", encoding=\"utf-8\").read()\n"
      ],
      "metadata": {
        "id": "6Rz6knJ3hilX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_case = text.lower()\n",
        "\n",
        "# Removing punctuations\n",
        "cleaned_text = lower_case.translate(str.maketrans('', '', string.punctuation))\n"
      ],
      "metadata": {
        "id": "dEOZLjvqlQ7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting text into words\n",
        "tokenized_words = cleaned_text.split()\n",
        "\n",
        "stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
        "              \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
        "              \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\n",
        "              \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\",\n",
        "              \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
        "              \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
        "              \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
        "              \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
        "              \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n",
        "              \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n"
      ],
      "metadata": {
        "id": "nM0hgfeQlX5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stop words from the tokenized words list\n",
        "final_words = []\n",
        "for word in tokenized_words:\n",
        "    if word not in stop_words:\n",
        "        final_words.append(word)"
      ],
      "metadata": {
        "id": "piO2VSW9lfLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_list = []\n",
        "with open('emotion.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        clear_line = line.replace(\"\\n\", '').replace(\",\", '').replace(\"'\", '').strip()\n",
        "        word, emotion = clear_line.split(':')\n",
        "\n",
        "        if word in final_words:\n",
        "            emotion_list.append(emotion)\n",
        "\n",
        "print(emotion_list)\n",
        "w = Counter(emotion_list)\n",
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9cScwfoliHi",
        "outputId": "fd5b47a5-b61a-4f0e-b1c2-5df512fbe912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' attracted', ' sad', ' adequate', ' sad']\n",
            "Counter({' sad': 2, ' attracted': 1, ' adequate': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "JNbkc3-PoWx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H23KV3uTogtf",
        "outputId": "809bd5ea-a749-43c5-90fe-b072eeac0818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer=PorterStemmer()\n",
        "input_str=(text)\n",
        "input_str=nltk.word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "  print(stemmer.stem(word))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uICgw4V3mNfz",
        "outputId": "a9f85420-3c6d-43ba-8c79-61fa5e3d7c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ï»¿the\n",
            "citi\n",
            "of\n",
            "zhengzhou\n",
            ",\n",
            "home\n",
            "to\n",
            "the\n",
            "world\n",
            "'s\n",
            "largest\n",
            "iphon\n",
            "manufactur\n",
            "plant\n",
            ",\n",
            "wa\n",
            "also\n",
            "affect\n",
            ".\n",
            "it\n",
            "come\n",
            "as\n",
            "china\n",
            "report\n",
            "a\n",
            "third\n",
            "straight\n",
            "day\n",
            "of\n",
            "more\n",
            "than\n",
            "1,000\n",
            "case\n",
            ".\n",
            "earlier\n",
            "thi\n",
            "month\n",
            "mr\n",
            "xi\n",
            "signal\n",
            "that\n",
            "there\n",
            "would\n",
            "be\n",
            "no\n",
            "eas\n",
            "up\n",
            "of\n",
            "the\n",
            "zero-covid\n",
            "polici\n",
            ",\n",
            "call\n",
            "it\n",
            "a\n",
            "``\n",
            "peopl\n",
            "'s\n",
            "war\n",
            "to\n",
            "stop\n",
            "the\n",
            "spread\n",
            "of\n",
            "the\n",
            "viru\n",
            "''\n",
            ".\n",
            "as\n",
            "of\n",
            "oct\n",
            "24\n",
            ",\n",
            "some\n",
            "28\n",
            "citi\n",
            "across\n",
            "the\n",
            "countri\n",
            "were\n",
            "implement\n",
            "some\n",
            "degre\n",
            "of\n",
            "lockdown\n",
            "measur\n",
            ",\n",
            "analyst\n",
            "nomura\n",
            "told\n",
            "news\n",
            "agenc\n",
            "reuter\n",
            "-\n",
            "with\n",
            "around\n",
            "207\n",
            "million\n",
            "peopl\n",
            "affect\n",
            "in\n",
            "region\n",
            "respons\n",
            "for\n",
            "almost\n",
            "a\n",
            "quarter\n",
            "of\n",
            "china\n",
            "'s\n",
            "gdp\n",
            ",\n",
            "it\n",
            "ad\n",
            ".\n",
            "across\n",
            "the\n",
            "countri\n",
            ",\n",
            "around\n",
            "200\n",
            "lockdown\n",
            "have\n",
            "been\n",
            "implement\n",
            "in\n",
            "recent\n",
            "day\n",
            "-\n",
            "the\n",
            "major\n",
            "of\n",
            "thi\n",
            "affect\n",
            "commun\n",
            "that\n",
            "have\n",
            "been\n",
            "mark\n",
            "as\n",
            "high\n",
            "or\n",
            "medium\n",
            "risk\n",
            ".\n",
            "resid\n",
            "in\n",
            "differ\n",
            "area\n",
            "are\n",
            "subject\n",
            "to\n",
            "differ\n",
            "rule\n",
            ",\n",
            "depend\n",
            "on\n",
            "whether\n",
            "they\n",
            "are\n",
            "in\n",
            "a\n",
            "low\n",
            ",\n",
            "medium\n",
            "or\n",
            "high-risk\n",
            "zone\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_jHhvO_oFtl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}